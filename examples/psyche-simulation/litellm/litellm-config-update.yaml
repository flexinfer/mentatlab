general_settings:
  enable_streaming: true
  global_max_parallel_requests: 10
  master_key: 90FcWdIeLIT
  max_parallel_requests: 10
litellm_settings:
  drop_params: true
  request_timeout: 600
  set_verbose: false
model_list:
- litellm_params:
    api_base: http://ollama-cluster.ai.svc.cluster.local:11434/v1
    api_key: dummy
    keep_alive: 300
    model: openai/mistral:7b-instruct-v0.3-q4_K_M
    model_type: openai
    num_batch: 512
    num_ctx: 8192
    num_gpu: 999
    num_thread: 8
    stream: true
  model_info:
    max_input_tokens: 8192
    max_output_tokens: 2000
  model_name: mistral-psyche
- litellm_params:
    api_base: http://ollama-cluster.ai.svc.cluster.local:11434
    api_key: ""
    drop_params: true
    keep_alive: "-1"
    model: ollama/nomic-embed-text:latest
    model_type: ollama
  model_info:
    is_embedding_model: true
  model_name: nomic-embed
- litellm_params:
    api_base: http://ollama-cluster.ai.svc.cluster.local:11434
    api_key: dummy
    keep_alive: 300
    model: ollama/huihui_ai/deepseek-r1-abliterated:8b
    model_type: ollama
    num_batch: 512
    num_ctx: 16384
    num_gpu: 999
    num_thread: 8
    stream: true
  model_info:
    max_input_tokens: 16384
    max_output_tokens: 4096
  model_name: deepseek-r1-abliterated
router_settings:
  default_max_parallel_requests: 10
  routing_strategy: least-busy